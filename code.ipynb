{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    \n",
    "    def _ship_related_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Splits 'Cabin' into 'Deck' and 'Cabin_part'.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing the 'Cabin' column.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with 'Deck' and 'Cabin_part' columns.\n",
    "        \"\"\"\n",
    "        # Split 'Cabin' column into 'Deck' and 'Cabin_part' directly\n",
    "        data[['Deck', 'Cabin_part']] = data['Cabin'].str.split(\"/\", expand=True).iloc[:, [0, 2]]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _passenger_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates passenger-related features including group size and family details.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing passenger information.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with additional passenger features.\n",
    "        \"\"\"\n",
    "        # Calculate the number of passengers per cabin and per group in one go using transform\n",
    "        data['NUMBER_OF_PASSENGERS_PER_CABIN'] = data.groupby('Cabin')['PassengerId'].transform('count')\n",
    "        data['NUMBER_OF_PASSENGERS_PER_GROUP'] = data['PassengerId'].str.split(\"_\").str[0].map(\n",
    "            data['PassengerId'].str.split(\"_\").str[0].value_counts()\n",
    "        )\n",
    "        data['Avg_Age_Per_Group'] = data.groupby('NUMBER_OF_PASSENGERS_PER_GROUP')['Age'].transform('mean')\n",
    "\n",
    "        # Create 'wasAlonePerGroup' and 'wasAlonePerCabin' columns based on conditions\n",
    "        data['wasAlonePerGroup'] = (data['NUMBER_OF_PASSENGERS_PER_GROUP'] == 1).astype(int)\n",
    "        data['wasAlonePerCabin'] = (data['NUMBER_OF_PASSENGERS_PER_CABIN'] == 1).astype(int)\n",
    "\n",
    "        # Extract last name directly and calculate family-related columns\n",
    "        data['LAST_NAME'] = data['Name'].str.split().str[1]\n",
    "        data['FAMILY'] = data['LAST_NAME'].map(data['LAST_NAME'].value_counts())\n",
    "        data['TRAVELLED_WITH_FAMILY'] = (data['FAMILY'] > 1).astype(int)\n",
    "\n",
    "        # Drop temporary columns if they are no longer needed\n",
    "        data.drop(columns=['FAMILY'], inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _service_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculates service-related spending features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing service spending information.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with service-related features.\n",
    "        \"\"\"\n",
    "        service_columns = ['RoomService', 'Spa', 'FoodCourt', 'ShoppingMall', 'VRDeck']\n",
    "\n",
    "        mean_spending_pass = data.groupby('PassengerId')[service_columns].mean()\n",
    "        median_spending_pass = data.groupby('PassengerId')[service_columns].median()\n",
    "        total_spending_pass = data.groupby('PassengerId')[service_columns].sum()\n",
    "\n",
    "        mean_spending_fam = data.groupby('LAST_NAME')[service_columns].mean()\n",
    "        median_spending_fam = data.groupby('LAST_NAME')[service_columns].median()\n",
    "        total_spending_fam = data.groupby('LAST_NAME')[service_columns].sum()\n",
    "\n",
    "\n",
    "        for column in service_columns:\n",
    "            data[f'Mean_Spending_On_{column}_Pass'] = data['PassengerId'].map(mean_spending_pass[column])\n",
    "            data[f'Median_Spending_On_{column}_Pass'] = data['PassengerId'].map(median_spending_pass[column])\n",
    "            data[f'Total_Spending_On_{column}_Pass'] = data['PassengerId'].map(total_spending_pass[column])\n",
    "\n",
    "            data[f'Mean_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(mean_spending_fam[column])\n",
    "            data[f'Median_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(median_spending_fam[column])\n",
    "            data[f'Total_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(total_spending_fam[column])\n",
    "\n",
    "        data['isMinor'] = (data['Age'] < 18).astype(int)\n",
    "        data['Age_Cat'] = data['Age'].apply(lambda x: 'Child' if x <= 12 else \n",
    "                                              'Teen' if x < 18 else \n",
    "                                              'Adult' if x < 64 else 'Senior')\n",
    "        data['isCyroSleep'] = data['CryoSleep'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "        data['isVIP'] = data['VIP'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "        data['Average_Family_Size'] = data.groupby('LAST_NAME')['PassengerId'].transform('count')\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def _handling_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handles missing values in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to handle missing values.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method is not yet implemented.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This method is not yet implemented!\")\n",
    "    \n",
    "    def _destination_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates destination related features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing destination related features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with destination related features. \"\"\"\n",
    "\n",
    "        data['Destination_Count_By_HomePlanet'] = data.groupby('Destination')['HomePlanet'].transform('count')\n",
    "\n",
    "        return data \n",
    "\n",
    "    def _feature_pipeline(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Runs the full feature engineering pipeline on data.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to process.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The processed DataFrame with all features engineered.\n",
    "        \"\"\"\n",
    "        data_processed = self._ship_related_features(data)\n",
    "        data_processed = self._passenger_features(data_processed)\n",
    "        data_processed = self._service_features(data_processed)\n",
    "        data_processed = self._destination_features(data_processed)\n",
    "\n",
    "        return data_processed\n",
    "    \n",
    "    def _data_split(self, data: pd.DataFrame, features: list[str], target: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Splits the data into features and target.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing features and target.\n",
    "            features (list[str]): The list of feature column names.\n",
    "            target (str): The name of the target column.\n",
    "\n",
    "        Returns:\n",
    "            tuple[pd.DataFrame, pd.Series]: A tuple containing the features DataFrame and target Series.\n",
    "        \"\"\"\n",
    "        X_train = data[features]\n",
    "        y_train = data[target]\n",
    "\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def _get_dummies(self, data: pd.DataFrame, dtype: type) -> pd.DataFrame:\n",
    "        \"\"\"Creates dummy variables for categorical features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to create dummy variables from.\n",
    "            dtype (type): The desired data type for the resulting dummy variables.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with dummy variables added.\n",
    "        \"\"\"\n",
    "        dummied_data = pd.get_dummies(data, dtype=dtype)\n",
    "        \n",
    "        return dummied_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "    \"\"\"\n",
    "    A class to handle machine learning model initialization and hyperparameter tuning using Optuna.\n",
    "\n",
    "    Attributes:\n",
    "        model (Any): The initialized machine learning model.\n",
    "        available_models (Dict[str, Any]): A dictionary mapping model names to their respective classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Modelling class and defines available models for initialization.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.available_models = {\n",
    "            'RandomForestClassifier': RandomForestClassifier, \n",
    "            'XGBClassifier': XGBClassifier,\n",
    "            'LGBMClassifier': LGBMClassifier,\n",
    "        }\n",
    "\n",
    "    def initialize_model(self, model_name: str, params: Optional[Dict[str, Any]] = None) -> Any:\n",
    "        \"\"\"\n",
    "        Initializes and returns a machine learning model based on the model_name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model to initialize.\n",
    "            params (Optional[Dict[str, Any]]): Hyperparameters for the model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Any: An instance of the selected model.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model_name is not in available_models.\n",
    "        \"\"\"\n",
    "        if model_name not in self.available_models:\n",
    "            print(f\"The {model_name} is not yet available, you can select a following model: {list(self.available_models.keys())}\")\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "        \n",
    "        \n",
    "        return self.available_models[model_name](**(params or {}))\n",
    "    \n",
    "    def predict_test_set(self, model: Any, X_test: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Predicts the target variable for the given test set using the provided model.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The trained model used for making predictions.\n",
    "            X_test (pd.DataFrame): The test set features for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted values for the test set.\n",
    "        \"\"\"\n",
    "        predictions = model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def fit_model(self, model: Any, X_train: pd.DataFrame, y_train: pd.Series) -> Any:\n",
    "        \"\"\"\n",
    "        Fits the provided model to the training data.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to be trained.\n",
    "            X_train (pd.DataFrame): The training set features.\n",
    "            y_train (pd.Series): The target variable corresponding to the training set features.\n",
    "\n",
    "        Returns:\n",
    "            Any: The fitted model.\n",
    "        \"\"\"\n",
    "        return model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "    def tune_model(self, model_name: str, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes the specified model's hyperparameters using Optuna.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model to tune.\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model_name is not in available_models.\n",
    "        \"\"\"\n",
    "\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        if model_name not in self.available_models:\n",
    "            print(f\"The {model_name} is not yet available, you can select a following model: {list(self.available_models.keys())}\")\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "\n",
    "        if model_name == 'RandomForestClassifier':\n",
    "            return self.tune_random_forest(X_train, y_train, n_trials)\n",
    "\n",
    "        elif model_name == 'XGBClassifier':\n",
    "            return self.tune_xgb(X_train, y_train, n_trials)\n",
    "\n",
    "        elif model_name == 'LGBMClassifier':\n",
    "            return self.tune_lgbm(X_train, y_train, n_trials)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "\n",
    "    def tune_random_forest(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the RandomForestClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "        def rfc_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "            bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "            criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "            max_leaf_nodes = trial.suggest_int(\"max_leaf_nodes\", 10, 100)\n",
    "\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                bootstrap=bootstrap,\n",
    "                criterion = criterion,\n",
    "                max_leaf_nodes = max_leaf_nodes,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            score = cross_val_score(rf, X_train, y_train, cv=5, scoring=\"accuracy\").mean()\n",
    "            return score\n",
    "\n",
    "        rfc_study = optuna.create_study(direction=\"maximize\")\n",
    "        rfc_study.optimize(rfc_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for RandomForestClassifier:\", rfc_study.best_params)\n",
    "        print(\"Best score for RandomForestClassifier:\", rfc_study.best_value)\n",
    "\n",
    "        return rfc_study.best_params\n",
    "\n",
    "    def tune_xgb(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the XGBClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "        def xgb_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "            subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 5)\n",
    "            reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 10)\n",
    "            reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "            scale_pos_weight = trial.suggest_float(\"scale_pos_weight\", 0.5, 1.0)\n",
    "\n",
    "            xgb = XGBClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                gamma=gamma,\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            score = cross_val_score(xgb, X_train, y_train, cv=5, scoring=\"accuracy\").mean()\n",
    "            return score\n",
    "\n",
    "        xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "        xgb_study.optimize(xgb_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for XGBClassifier:\", xgb_study.best_params)\n",
    "        print(\"Best score for XGBClassifier:\", xgb_study.best_value)\n",
    "\n",
    "        return xgb_study.best_params\n",
    "\n",
    "    def tune_lgbm(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the LGBMClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "        def lgbm_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", -1, 15)  # -1 means no limit\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "            num_leaves = trial.suggest_int(\"num_leaves\", 2, 256)\n",
    "            min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "            subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "            reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 10)\n",
    "            reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "\n",
    "            lgbm = LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                num_leaves=num_leaves,\n",
    "                min_child_samples=min_child_samples,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            score = cross_val_score(lgbm, X_train, y_train, cv=5, scoring=\"accuracy\").mean()\n",
    "            return score\n",
    "\n",
    "        lgbm_study = optuna.create_study(direction=\"maximize\")\n",
    "        lgbm_study.optimize(lgbm_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for LGBMClassifier:\", lgbm_study.best_params)\n",
    "        print(\"Best score for LGBMClassifier:\", lgbm_study.best_value)\n",
    "\n",
    "        return lgbm_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = ['Deck', 'Cabin_part', 'NUMBER_OF_PASSENGERS_PER_CABIN', 'NUMBER_OF_PASSENGERS_PER_GROUP', 'wasAlonePerGroup', 'wasAlonePerCabin', 'TRAVELLED_WITH_FAMILY', 'isMinor', 'Age_Cat',\n",
    "       'Mean_Spending_On_RoomService_Pass', 'Mean_Spending_On_Spa_Pass',\n",
    "       'Mean_Spending_On_FoodCourt_Pass', 'Mean_Spending_On_ShoppingMall_Pass',\n",
    "       'Mean_Spending_On_VRDeck_Pass' ,'Mean_Spending_On_RoomService_Fam', 'Mean_Spending_On_Spa_Fam',\n",
    "       'Mean_Spending_On_FoodCourt_Fam', 'Mean_Spending_On_ShoppingMall_Fam',\n",
    "       'Mean_Spending_On_VRDeck_Fam','isCyroSleep', 'HomePlanet', 'isVIP', 'Destination', 'Destination_Count_By_HomePlanet',]\n",
    "TARGET = ['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "feature_engineering = FeatureEngineering()\n",
    "modelling = Modelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "processed_train = feature_engineering._feature_pipeline(train)\n",
    "processed_test = feature_engineering._feature_pipeline(test)\n",
    "X_train, y_train = feature_engineering._data_split(processed_train, features = FEATURES, target = TARGET)\n",
    "\n",
    "X_train = pd.get_dummies(X_train, dtype='int')\n",
    "y_train = pd.get_dummies(y_train, dtype='int')\n",
    "X_test = pd.get_dummies(processed_test[FEATURES], dtype='int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lgb_best_optuna_params = modelling.tune_model('LGBMClassifier', X_train=X_train, y_train=y_train, n_trials=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "LGB_MODEL = modelling.initialize_model(model_name='LGBMClassifier', params=lgb_best_optuna_params)\n",
    "LGB_MODEL_fitted = modelling.fit_model(model = LGB_MODEL, X_train=X_train, y_train=y_train)\n",
    "\n",
    "test['Transported'] = modelling.predict_test_set(model = LGB_MODEL_fitted, X_test=X_test)\n",
    "test['Transported'] = test['Transported'].apply(lambda x: True if x == 1 else False)\n",
    "submission = test[['PassengerId', 'Transported']]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
