{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    \n",
    "    def ship_related_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Splits 'Cabin' into 'Deck' and 'Cabin_part'.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing the 'Cabin' column.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with 'Deck' and 'Cabin_part' columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split 'Cabin' column by '/' and assign first and third parts to 'Deck' and 'Cabin_part'\n",
    "        data[['Deck', 'Cabin_part']] = data['Cabin'].str.split(\"/\", expand=True).iloc[:, [0, 2]]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def passenger_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates passenger-related features including group size and family details.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing passenger information.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with additional passenger features.\n",
    "        \"\"\"\n",
    "        # Count number of passengers per 'Cabin' and assign result to a new column\n",
    "        data['NUMBER_OF_PASSENGERS_PER_CABIN'] = data.groupby('Cabin')['PassengerId'].transform('count')\n",
    "\n",
    "        # Count passengers per group by splitting 'PassengerId' and assigning result to a new column\n",
    "        data['NUMBER_OF_PASSENGERS_PER_GROUP'] = data['PassengerId'].str.split(\"_\").str[0].map(\n",
    "            data['PassengerId'].str.split(\"_\").str[0].value_counts()\n",
    "        )\n",
    "\n",
    "        # Calculate mean age per group and assign to a new column\n",
    "        data['Avg_Age_Per_Group'] = data.groupby('NUMBER_OF_PASSENGERS_PER_GROUP')['Age'].transform('mean')\n",
    "\n",
    "        # Create binary columns indicating if the passenger is alone in their group or cabin\n",
    "        data['wasAlonePerGroup'] = (data['NUMBER_OF_PASSENGERS_PER_GROUP'] == 1).astype(int)\n",
    "        data['wasAlonePerCabin'] = (data['NUMBER_OF_PASSENGERS_PER_CABIN'] == 1).astype(int)\n",
    "\n",
    "        # Extract last name from 'Name' and assign to a new column\n",
    "        data['LAST_NAME'] = data['Name'].str.split().str[1]\n",
    "\n",
    "        # Count passengers with the same last name (family size) and assign to a temporary column\n",
    "        data['FAMILY'] = data['LAST_NAME'].map(data['LAST_NAME'].value_counts())\n",
    "\n",
    "        # Create binary column to indicate if the passenger traveled with family\n",
    "        data['TRAVELLED_WITH_FAMILY'] = (data['FAMILY'] > 1).astype(int)\n",
    "\n",
    "        # Remove the temporary 'FAMILY' column as itâ€™s no longer needed\n",
    "        data.drop(columns=['FAMILY'], inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def service_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculates service-related spending features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing service spending information.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with service-related features.\n",
    "        \"\"\"\n",
    "        # List of columns related to service spending\n",
    "        service_columns = ['RoomService', 'Spa', 'FoodCourt', 'ShoppingMall', 'VRDeck']\n",
    "\n",
    "        # Calculate mean, median, and total spending per passenger\n",
    "        mean_spending_pass = data.groupby('PassengerId')[service_columns].mean()\n",
    "        median_spending_pass = data.groupby('PassengerId')[service_columns].median()\n",
    "        total_spending_pass = data.groupby('PassengerId')[service_columns].sum()\n",
    "\n",
    "        # Calculate mean, median, and total spending per family based on last name\n",
    "        mean_spending_fam = data.groupby('LAST_NAME')[service_columns].mean()\n",
    "        median_spending_fam = data.groupby('LAST_NAME')[service_columns].median()\n",
    "        total_spending_fam = data.groupby('LAST_NAME')[service_columns].sum()\n",
    "\n",
    "        # Map spending statistics to each passenger for both passenger-based and family-based spending\n",
    "        for column in service_columns:\n",
    "            # Per passenger\n",
    "            data[f'Mean_Spending_On_{column}_Pass'] = data['PassengerId'].map(mean_spending_pass[column])\n",
    "            data[f'Median_Spending_On_{column}_Pass'] = data['PassengerId'].map(median_spending_pass[column])\n",
    "            data[f'Total_Spending_On_{column}_Pass'] = data['PassengerId'].map(total_spending_pass[column])\n",
    "\n",
    "            # Per family\n",
    "            data[f'Mean_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(mean_spending_fam[column])\n",
    "            data[f'Median_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(median_spending_fam[column])\n",
    "            data[f'Total_Spending_On_{column}_Fam'] = data['LAST_NAME'].map(total_spending_fam[column])\n",
    "\n",
    "        # Create binary columns for age and category indicators\n",
    "        data['isMinor'] = (data['Age'] < 18).astype(int)\n",
    "        data['Age_Cat'] = data['Age'].apply(lambda x: 'Child' if x <= 12 else \n",
    "                                              'Teen' if x < 18 else \n",
    "                                              'Adult' if x < 64 else 'Senior')\n",
    "\n",
    "        # Create binary columns indicating if the passenger is in CryoSleep or is VIP\n",
    "        data['isCyroSleep'] = data['CryoSleep'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "        data['isVIP'] = data['VIP'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "\n",
    "        # Calculate and add family size for each passenger based on last name\n",
    "        data['Average_Family_Size'] = data.groupby('LAST_NAME')['PassengerId'].transform('count')\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def handling_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handles missing values in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to handle missing values.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method is not yet implemented.\n",
    "        \"\"\"\n",
    "        # Raise an error to indicate the function has not yet been implemented\n",
    "        raise NotImplementedError(\"This method is not yet implemented!\")\n",
    "    \n",
    "    def destination_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates destination-related features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing destination related features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with destination related features.\n",
    "        \"\"\"\n",
    "        # Count the number of occurrences of each 'Destination' by 'HomePlanet'\n",
    "        data['Destination_Count_By_HomePlanet'] = data.groupby('Destination')['HomePlanet'].transform('count')\n",
    "\n",
    "        return data \n",
    "\n",
    "    \n",
    "    def missing_value_imputation(self, data: pd.DataFrame, n_neighbors: int, columns_to_impute: list) -> pd.DataFrame:\n",
    "        \"\"\"Imputes missing values using the KNNImputer.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing missing values.\n",
    "            n_neighbors (int): Number of neighbors for KNNImputer.\n",
    "            columns_to_impute (list): Columns to be imputed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with missing values imputed.\n",
    "        \"\"\"\n",
    "        # Initialize the KNN imputer with the specified number of neighbors\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "        # Select only the columns to be imputed\n",
    "        data_to_impute = data[columns_to_impute]\n",
    "\n",
    "        # Fit and transform the data for imputation, then update original data\n",
    "        data_imputed = pd.DataFrame(imputer.fit_transform(data_to_impute), columns=columns_to_impute, index=data.index)\n",
    "        data.update(data_imputed)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def feature_pipeline(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Runs the full feature engineering pipeline on data.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to process.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The processed DataFrame with all features engineered.\n",
    "        \"\"\"\n",
    "        # Apply each feature engineering function to the data\n",
    "        data_processed = self.ship_related_features(data)\n",
    "        data_processed = self.passenger_features(data_processed)\n",
    "        data_processed = self.service_features(data_processed)\n",
    "        data_processed = self.destination_features(data_processed)\n",
    "\n",
    "        return data_processed\n",
    "    \n",
    "    def data_split(self, data: pd.DataFrame, features: list[str], target: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Splits the data into features and target.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing features and target.\n",
    "            features (list[str]): The list of feature column names.\n",
    "            target (str): The name of the target column.\n",
    "\n",
    "        Returns:\n",
    "            tuple[pd.DataFrame, pd.Series]: A tuple containing the features DataFrame and target Series.\n",
    "        \"\"\"\n",
    "        # Select the feature columns and target column from data\n",
    "        X_train = data[features]\n",
    "        y_train = data[target]\n",
    "\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def get_dummies(self, data: pd.DataFrame, dtype: type) -> pd.DataFrame:\n",
    "        \"\"\"Creates dummy variables for categorical features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame to create dummy variables from.\n",
    "            dtype (type): The desired data type for the resulting dummy variables.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with dummy variables added.\n",
    "        \"\"\"\n",
    "        # Convert categorical columns to dummy/one-hot encoded columns\n",
    "        dummied_data = pd.get_dummies(data, dtype=dtype)\n",
    "        \n",
    "        return dummied_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "    \"\"\"\n",
    "    A class to handle machine learning model initialization and hyperparameter tuning using Optuna.\n",
    "\n",
    "    Attributes:\n",
    "        model (Any): The initialized machine learning model.\n",
    "        available_models (Dict[str, Any]): A dictionary mapping model names to their respective classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Modelling class and defines available models for initialization.\n",
    "        \"\"\"\n",
    "        # No model initialized yet\n",
    "        self.model = None\n",
    "\n",
    "        # Dictionary to hold available model classes for initialization\n",
    "        self.available_models = {\n",
    "            'RandomForestClassifier': RandomForestClassifier, \n",
    "            'XGBClassifier': XGBClassifier,\n",
    "            'LGBMClassifier': LGBMClassifier,\n",
    "            'HistGradientBoostingClassifier': HistGradientBoostingClassifier,\n",
    "        }\n",
    "\n",
    "\n",
    "    def initialize_model(self, model_name: str, params: Optional[Dict[str, Any]] = None) -> Any:\n",
    "        \"\"\"\n",
    "        Initializes and returns a machine learning model based on the model_name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model to initialize.\n",
    "            params (Optional[Dict[str, Any]]): Hyperparameters for the model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Any: An instance of the selected model.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model_name is not in available_models.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the model name is valid\n",
    "        if model_name not in self.available_models:\n",
    "            print(f\"The {model_name} is not yet available, you can select a following model: {list(self.available_models.keys())}\")\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "        \n",
    "        # Initialize and return the model with given parameters, or empty dictionary if None\n",
    "        return self.available_models[model_name](**(params or {}))\n",
    "    \n",
    "    def predict_test_set(self, model: Any, X_test: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Predicts the target variable for the given test set using the provided model.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The trained model used for making predictions.\n",
    "            X_test (pd.DataFrame): The test set features for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted values for the test set.\n",
    "        \"\"\"\n",
    "        predictions = model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def fit_model(self, model: Any, X_train: pd.DataFrame, y_train: pd.Series) -> Any:\n",
    "        \"\"\"\n",
    "        Fits the provided model to the training data.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to be trained.\n",
    "            X_train (pd.DataFrame): The training set features.\n",
    "            y_train (pd.Series): The target variable corresponding to the training set features.\n",
    "\n",
    "        Returns:\n",
    "            Any: The fitted model.\n",
    "        \"\"\"\n",
    "        return model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "    def tune_model(self, model_name: str, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes the specified model's hyperparameters using Optuna.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model to tune.\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model_name is not in available_models.\n",
    "        \"\"\"\n",
    "\n",
    "        # Suppress Optuna's verbose logging\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        # Check if the model name is valid\n",
    "        if model_name not in self.available_models:\n",
    "            print(f\"The {model_name} is not yet available, you can select a following model: {list(self.available_models.keys())}\")\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "\n",
    "        # Choose the appropriate tuning method based on model name\n",
    "        if model_name == 'RandomForestClassifier':\n",
    "            return self.tune_random_forest(X_train, y_train, n_trials)\n",
    "\n",
    "        elif model_name == 'XGBClassifier':\n",
    "            return self.tune_xgb(X_train, y_train, n_trials)\n",
    "\n",
    "        elif model_name == 'LGBMClassifier':\n",
    "            return self.tune_lgbm(X_train, y_train, n_trials)\n",
    "        \n",
    "        elif model_name == 'HistGradientBoostingClassifier':\n",
    "            return self.tune_hgbc(X_train, y_train, n_trials)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{model_name} is not a valid model.\")\n",
    "\n",
    "    def tune_random_forest(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the RandomForestClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define hyperparameters to tune\n",
    "        def rfc_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "            bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "            criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "            max_leaf_nodes = trial.suggest_int(\"max_leaf_nodes\", 10, 100)\n",
    "\n",
    "            # Initialize model with suggested hyperparameters\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                bootstrap=bootstrap,\n",
    "                criterion = criterion,\n",
    "                max_leaf_nodes = max_leaf_nodes,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "             # Perform cross-validation and return mean accuracy\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            score = cross_val_score(rf, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs = -1).mean()\n",
    "            return score\n",
    "\n",
    "        # Create and run an Optuna study\n",
    "        rfc_study = optuna.create_study(direction=\"maximize\")\n",
    "        rfc_study.optimize(rfc_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for RandomForestClassifier:\", rfc_study.best_params)\n",
    "        print(\"Best score for RandomForestClassifier:\", rfc_study.best_value)\n",
    "\n",
    "        return rfc_study.best_params\n",
    "\n",
    "    def tune_xgb(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the XGBClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "        def xgb_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "            subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 5)\n",
    "            reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 10)\n",
    "            reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "            scale_pos_weight = trial.suggest_float(\"scale_pos_weight\", 0.5, 1.0)\n",
    "            min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "            \n",
    "            xgb = XGBClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                gamma=gamma,\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                min_child_weight=min_child_weight,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            score = cross_val_score(xgb, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs = -1).mean()\n",
    "            return score\n",
    "\n",
    "        xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "        xgb_study.optimize(xgb_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for XGBClassifier:\", xgb_study.best_params)\n",
    "        print(\"Best score for XGBClassifier:\", xgb_study.best_value)\n",
    "\n",
    "        return xgb_study.best_params\n",
    "\n",
    "    def tune_lgbm(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the LGBMClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "        def lgbm_objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", -1, 15)  # -1 means no limit\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "            num_leaves = trial.suggest_int(\"num_leaves\", 2, 256)\n",
    "            min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "            subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "            reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 10)\n",
    "            reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "\n",
    "            lgbm = LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                num_leaves=num_leaves,\n",
    "                min_child_samples=min_child_samples,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            score = cross_val_score(lgbm, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs = -1).mean()\n",
    "            return score\n",
    "\n",
    "        lgbm_study = optuna.create_study(direction=\"maximize\")\n",
    "        lgbm_study.optimize(lgbm_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for LGBMClassifier:\", lgbm_study.best_params)\n",
    "        print(\"Best score for LGBMClassifier:\", lgbm_study.best_value)\n",
    "\n",
    "        return lgbm_study.best_params\n",
    "    \n",
    "    def tune_hgbc(self, X_train: Any, y_train: Any, n_trials: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tunes hyperparameters for the HGBClassifier.\n",
    "\n",
    "        Args:\n",
    "            X_train (Any): Training features.\n",
    "            y_train (Any): Training labels.\n",
    "            n_trials (int): Number of trials for hyperparameter tuning.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: The best parameters found during tuning.\n",
    "        \"\"\"\n",
    "\n",
    "        def hgbc_objective(trial):\n",
    "\n",
    "            learning_rate =  trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "            max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "            max_leaf_nodes = trial.suggest_int(\"max_leaf_nodes\", 10, 255)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 10, 200)\n",
    "            l2_regularization = trial.suggest_float(\"l2_regularization\", 0.0, 10.0)\n",
    "            max_bins = trial.suggest_int(\"max_bins\", 50, 255)\n",
    "            early_stopping = trial.suggest_categorical(\"early_stopping\", [True, False])\n",
    "            scoring = \"accuracy\"  # Setting scoring to accuracy\n",
    "\n",
    "            hgbc = HistGradientBoostingClassifier (\n",
    "                learning_rate=learning_rate,\n",
    "                max_iter=max_iter,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                l2_regularization=l2_regularization,\n",
    "                max_bins=max_bins,\n",
    "                early_stopping=early_stopping,\n",
    "                scoring=scoring)\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            score = cross_val_score(hgbc, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs = -1).mean()\n",
    "            return score\n",
    "\n",
    "        hgbc_study = optuna.create_study(direction=\"maximize\")\n",
    "        hgbc_study.optimize(hgbc_objective, n_trials=n_trials)\n",
    "\n",
    "        print(\"Best parameters for HGBClassifier:\", hgbc_study.best_params)\n",
    "        print(\"Best score for HGBClassifier:\", hgbc_study.best_value)\n",
    "\n",
    "        return hgbc_study.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['Deck', 'Cabin_part', 'NUMBER_OF_PASSENGERS_PER_CABIN', 'NUMBER_OF_PASSENGERS_PER_GROUP', 'wasAlonePerGroup', 'wasAlonePerCabin', 'TRAVELLED_WITH_FAMILY', 'isMinor', 'Age_Cat',\n",
    "       'Mean_Spending_On_RoomService_Pass', 'Mean_Spending_On_Spa_Pass',\n",
    "       'Mean_Spending_On_FoodCourt_Pass', 'Mean_Spending_On_ShoppingMall_Pass',\n",
    "       'Mean_Spending_On_VRDeck_Pass' ,'Mean_Spending_On_RoomService_Fam', 'Mean_Spending_On_Spa_Fam',\n",
    "       'Mean_Spending_On_FoodCourt_Fam', 'Mean_Spending_On_ShoppingMall_Fam',\n",
    "       'Mean_Spending_On_VRDeck_Fam','isCyroSleep', 'HomePlanet', 'isVIP', 'Destination', 'Destination_Count_By_HomePlanet']\n",
    "TARGET = ['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = FeatureEngineering()\n",
    "modelling = Modelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train = feature_engineering.missing_value_imputation(train, 5, ['Spa', 'RoomService', 'FoodCourt', 'ShoppingMall', 'VRDeck'])\n",
    "processed_train = feature_engineering.feature_pipeline(imputed_train)\n",
    "\n",
    "imputed_test = feature_engineering.missing_value_imputation(test, 5, ['Spa', 'RoomService', 'FoodCourt', 'ShoppingMall', 'VRDeck'])\n",
    "processed_test = feature_engineering.feature_pipeline(imputed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering.data_split(processed_train, features = FEATURES, target = TARGET)\n",
    "\n",
    "X_train = pd.get_dummies(X_train, dtype='int')\n",
    "y_train = pd.get_dummies(y_train, dtype='int')\n",
    "X_test = pd.get_dummies(processed_test[FEATURES], dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_optuna_params = modelling.tune_model('XGBClassifier', X_train=X_train, y_train=y_train, n_trials=300)\n",
    "rfr_best_optuna_params = modelling.tune_model('RandomForestClassifier', X_train=X_train, y_train=y_train, n_trials=300)\n",
    "lgbm_best_optuna_params = modelling.tune_model('LGBMClassifier', X_train=X_train, y_train=y_train, n_trials=300)\n",
    "hgbc_best_optuna_params = modelling.tune_model('HistGradientBoostingClassifier', X_train=X_train, y_train=y_train, n_trials=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(**rfr_best_optuna_params)\n",
    "clf2 = LGBMClassifier(**lgbm_best_optuna_params)\n",
    "clf3 = XGBClassifier(**xgb_best_optuna_params)\n",
    "clf4 = HistGradientBoostingClassifier(**hgbc_best_optuna_params)\n",
    "eclf1 = VotingClassifier(estimators=[('rfr', clf1), ('lgb', clf2), ('xgb', clf3), ('hgbc', clf4)])\n",
    "eclf1 = eclf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Transported'] = modelling.predict_test_set(model = eclf1, X_test=X_test)\n",
    "test['Transported'] = test['Transported'].apply(lambda x: True if x == 1 else False)\n",
    "submission = test[['PassengerId', 'Transported']]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
